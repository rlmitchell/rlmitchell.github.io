{"categories":[{"title":"ansible","uri":"https://rlmitchell.github.io/categories/ansible/"},{"title":"aws","uri":"https://rlmitchell.github.io/categories/aws/"},{"title":"bash","uri":"https://rlmitchell.github.io/categories/bash/"},{"title":"boto3","uri":"https://rlmitchell.github.io/categories/boto3/"},{"title":"markdown","uri":"https://rlmitchell.github.io/categories/markdown/"},{"title":"python","uri":"https://rlmitchell.github.io/categories/python/"},{"title":"rds","uri":"https://rlmitchell.github.io/categories/rds/"},{"title":"s3","uri":"https://rlmitchell.github.io/categories/s3/"},{"title":"ses","uri":"https://rlmitchell.github.io/categories/ses/"}],"posts":[{"content":"Requirements: The best way I\u0026rsquo;ve found to open more than one VSCode instances in linux is to run them under separate users. Problem is my Ubuntu 20.04 lightdm distribution is configured to nolisten out of the box. Here\u0026rsquo;s how to enable connections.\nLightdm configuration file If it doesn\u0026rsquo;t already exist, create /etc/lightdm/lightdm.conf\n[SeatDefaults] xserver-allow-tcp=true [XDMCPServer] enabled=true  Once the file is in place restart lightdm or reboot.\nVerify Xorg is listening You should see a -listen argument in the Xorg program\u0026rsquo;s arg list:\n$ ps -ef |grep Xorg root 639 597 11 11:52 tty7 00:06:03 /usr/lib/xorg/Xorg -core :0 -seat seat0 -auth /var/run/lightdm/root/:0 -listen tcp vt7 -novtswitch $  Allow connection Next allow X11 connections to your display from your ip address.\n$ id uid=1000(rob) gid=1000(rob) groups=1000(rob),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),121(lpadmin),131(lxd),132(sambashare) $ xhost +192.168.1.83 192.168.1.83 being added to access control list $  Verify X11 connection with a separate account Finally, under a separate account, startup VSCode.\n$ id uid=1001(blog) gid=1001(blog) groups=1001(blog) $ export DISPLAY=:0.0 $ code .  ","id":0,"section":"posts","summary":"Requirements: The best way I\u0026rsquo;ve found to open more than one VSCode instances in linux is to run them under separate users. Problem is my Ubuntu 20.04 lightdm distribution is configured to nolisten out of the box. Here\u0026rsquo;s how to enable connections.\nLightdm configuration file If it doesn\u0026rsquo;t already exist, create /etc/lightdm/lightdm.conf\n[SeatDefaults] xserver-allow-tcp=true [XDMCPServer] enabled=true  Once the file is in place restart lightdm or reboot.\nVerify Xorg is listening You should see a -listen argument in the Xorg program\u0026rsquo;s arg list:","tags":null,"title":"Disabling nolisten in lightdm on Ubuntu 20.04","uri":"https://rlmitchell.github.io/2022/11/2022.11.15-ubuntu-2004-disable-nolisten/","year":"2022"},{"content":"  import boto3 from pprint import pprint class AWSRDSManualSnapshotPurger: def __init__(self, DBIdentifier, access_key=None, secret_key=None): if not access_key: self.client = boto3.client('rds') else: self.client = boto3.client('rds', region_name = 'us-east-1', aws_access_key_id = access_key, aws_secret_access_key = secret_key) self.db_identifier = DBIdentifier self.number_to_keep = 3 self.manual_snapshots_list = self.get_snapshots_list() def get_snapshots_list(self): manual_snapshots = [] for snapshot in self.client.describe_db_snapshots(SnapshotType='manual',DBInstanceIdentifier=self.db_identifier).get('DBSnapshots', None): manual_snapshots.append( (snapshot['SnapshotCreateTime'], snapshot['DBSnapshotIdentifier']) ) manual_snapshots.sort() return manual_snapshots def purge_snapshots(self): while len(self.manual_snapshots_list) \u0026gt; self.number_to_keep: del_snapshot = self.manual_snapshots_list.pop(0) print('delete: ' + str(del_snapshot)) #self.client.delete_db_snapshot(DBSnapshotIdentifier=del_snapshot[1]) if __name__ == '__main__': AWSRDSManualSnapshotPurger(DBIdentifier='SUPERDB').purge_snapshots()   Output:\n$ python3 snapshot-purge.py delete: (datetime.datetime(2022, 3, 7, 15, 21, 54, 836000, tzinfo=tzutc()), 'SUPERDB-20220307') delete: (datetime.datetime(2022, 10, 13, 0, 1, 27, 724000, tzinfo=tzutc()), 'SUPERDB-20221012') delete: (datetime.datetime(2022, 10, 25, 0, 1, 24, 425000, tzinfo=tzutc()), 'SUPERDB-20221024') $  ","id":1,"section":"posts","summary":"import boto3 from pprint import pprint class AWSRDSManualSnapshotPurger: def __init__(self, DBIdentifier, access_key=None, secret_key=None): if not access_key: self.client = boto3.client('rds') else: self.client = boto3.client('rds', region_name = 'us-east-1', aws_access_key_id = access_key, aws_secret_access_key = secret_key) self.db_identifier = DBIdentifier self.number_to_keep = 3 self.manual_snapshots_list = self.get_snapshots_list() def get_snapshots_list(self): manual_snapshots = [] for snapshot in self.client.describe_db_snapshots(SnapshotType='manual',DBInstanceIdentifier=self.db_identifier).get('DBSnapshots', None): manual_snapshots.append( (snapshot['SnapshotCreateTime'], snapshot['DBSnapshotIdentifier']) ) manual_snapshots.sort() return manual_snapshots def purge_snapshots(self): while len(self.manual_snapshots_list) \u0026gt; self.number_to_keep: del_snapshot = self.manual_snapshots_list.pop(0) print('delete: ' + str(del_snapshot)) #self.","tags":[],"title":"Purging old manual snapshots in AWS RDS with Python","uri":"https://rlmitchell.github.io/2022/11/2022.11.11-purge-old-rds-manual-snapshots-with-boto3/","year":"2022"},{"content":" Getting send quota with Python import boto3 from pprint import pprint class AWSSESQuota: def __init__(self, access_key=None, secret_key=None): self.threshold = 5000 if not access_key: self.client = boto3.client('ses') else: self.client = boto3.client('ses', region_name = 'us-east-1', aws_access_key_id = access_key, aws_secret_access_key = secret_key) def get_sent(self): return self.client.get_send_quota()['SentLast24Hours'] def over_threshold(self): return int(self.get_sent()) \u0026gt; self.threshold if __name__ == '__main__': quota = AWSSESQuota() pprint(quota.get_sent()) pprint(quota.over_threshold())   Example output:\n$ python3 quota-example.py 482.0 False $   Getting send quota with AWSCLI $ aws ses get-send-quota --region us-east-1 { \u0026quot;Max24HourSend\u0026quot;: 50000.0, \u0026quot;MaxSendRate\u0026quot;: 14.0, \u0026quot;SentLast24Hours\u0026quot;: 15.0 } $   Using unittest to test/check SES usage threshold from quota-example import AWSSESQuota import unittest class Test_SESQuotaThreshold(unittest.TestCase): def test_ses_usage(self): self.assertFalse(AWSSESQuota().over_threshold())   Output:\n$ python3 -m unittest test_ses_quota_threshold.py . ---------------------------------------------------------------------- Ran 1 test in 0.238s OK $  ","id":2,"section":"posts","summary":"Getting send quota with Python import boto3 from pprint import pprint class AWSSESQuota: def __init__(self, access_key=None, secret_key=None): self.threshold = 5000 if not access_key: self.client = boto3.client('ses') else: self.client = boto3.client('ses', region_name = 'us-east-1', aws_access_key_id = access_key, aws_secret_access_key = secret_key) def get_sent(self): return self.client.get_send_quota()['SentLast24Hours'] def over_threshold(self): return int(self.get_sent()) \u0026gt; self.threshold if __name__ == '__main__': quota = AWSSESQuota() pprint(quota.get_sent()) pprint(quota.over_threshold())   Example output:\n$ python3 quota-example.py 482.0 False $   Getting send quota with AWSCLI $ aws ses get-send-quota --region us-east-1 { \u0026quot;Max24HourSend\u0026quot;: 50000.","tags":[],"title":"Getting send quota in AWS SES with Python","uri":"https://rlmitchell.github.io/2022/11/2022.11.11-aws-ses-get-quota/","year":"2022"},{"content":" Here\u0026rsquo;s how you can get all the service names and associated service codes in AWS. The code also shows how to page through the output easily.\n import boto3 from pprint import pprint services = [] client = boto3.client('service-quotas') response = client.list_services(MaxResults=100) next_token = response.get('NextToken') services += response['Services'] while next_token: response = client.list_services(NextToken=next_token,MaxResults=100) services += response['Services'] next_token = response.get('NextToken',False) #pprint(services) pprint(services[:4]) #limit the example output   Output:\n[{'ServiceCode': 'AWSCloudMap', 'ServiceName': 'AWS Cloud Map'}, {'ServiceCode': 'a4b', 'ServiceName': 'Alexa for Business'}, {'ServiceCode': 'access-analyzer', 'ServiceName': 'Access Analyzer'}, {'ServiceCode': 'account', 'ServiceName': 'AWS Account Management'}]   More formally:\nimport boto3 from pprint import pprint class AWSAllServices: def __init__(self, access_key=None, secret_key=None): self.services = [] if not access_key: self.client = boto3.client('service-quotas') else: self.client = boto3.client('service-quotas', region_name = 'us-east-1', aws_access_key_id = access_key, aws_secret_access_key = secret_key) def __call__(self): return self.get_services() def get_services(self): response = self.client.list_services(MaxResults=100) next_token = response.get('NextToken') self.services += response['Services'] while next_token: response = self.client.list_services(NextToken=next_token,MaxResults=100) self.services += response['Services'] next_token = response.get('NextToken',False) return self.services if __name__ == '__main__': pprint(AWSAllServices()())  ","id":3,"section":"posts","summary":"Here\u0026rsquo;s how you can get all the service names and associated service codes in AWS. The code also shows how to page through the output easily.\n import boto3 from pprint import pprint services = [] client = boto3.client('service-quotas') response = client.list_services(MaxResults=100) next_token = response.get('NextToken') services += response['Services'] while next_token: response = client.list_services(NextToken=next_token,MaxResults=100) services += response['Services'] next_token = response.get('NextToken',False) #pprint(services) pprint(services[:4]) #limit the example output   Output:\n[{'ServiceCode': 'AWSCloudMap', 'ServiceName': 'AWS Cloud Map'}, {'ServiceCode': 'a4b', 'ServiceName': 'Alexa for Business'}, {'ServiceCode': 'access-analyzer', 'ServiceName': 'Access Analyzer'}, {'ServiceCode': 'account', 'ServiceName': 'AWS Account Management'}]   More formally:","tags":[],"title":"Listing all services and codes in AWS with Python","uri":"https://rlmitchell.github.io/2022/11/2022.11.11-aws-get-all-services-listing/","year":"2022"},{"content":" To leave a blank line in markdown simply leave a blank line followed by a line with only \u0026amp;nbsp; html spacing then followed by another blank line.\n Works in AWS CodeCommit markdown and Hugo markdown.\n Example:\nSome text here \u0026amp;nbsp; Some more text here  ","id":4,"section":"posts","summary":" To leave a blank line in markdown simply leave a blank line followed by a line with only \u0026amp;nbsp; html spacing then followed by another blank line.\n Works in AWS CodeCommit markdown and Hugo markdown.\n Example:\nSome text here \u0026amp;nbsp; Some more text here  ","tags":[],"title":"Blank lines in markdown","uri":"https://rlmitchell.github.io/2022/11/2022.11.11-markdown-blank-lines/","year":"2022"},{"content":" Requirements  boto3   RDSClient class class RDSClient: def __init__(self, DBIdentifier=None, access_key=None, secret_key=None, region=None): if not access_key: self.client = boto3.client('rds') else: self.client = boto3.client('rds', region_name = region, aws_access_key_id = access_key, aws_secret_access_key = secret_key) self.db_identifier = DBIdentifier def get_manual_snapshots(self): if self.db_identifier == None: return None manual_snapshots = [] for snapshot in self.client.describe_db_snapshots(SnapshotType='manual',DBInstanceIdentifier=self.db_identifier).get('DBSnapshots', None): manual_snapshots.append( (snapshot['SnapshotCreateTime'], snapshot['DBSnapshotIdentifier']) ) manual_snapshots.sort() return manual_snapshots def get_instance_names(self): instance_name_list = [] instances = self.client.describe_db_instances().get('DBInstances', None) #MaxRecords defaults to 100 for instance in instances: instance_name_list.append( instance['DBInstanceIdentifier'] ) return tuple(set(instance_name_list)) def delete_manual_snapshot(self, snapshot_identifier): response = self.client.delete_db_snapshot(DBSnapshotIdentifier=snapshot_identifier) return response  ","id":5,"section":"posts","summary":"Requirements  boto3   RDSClient class class RDSClient: def __init__(self, DBIdentifier=None, access_key=None, secret_key=None, region=None): if not access_key: self.client = boto3.client('rds') else: self.client = boto3.client('rds', region_name = region, aws_access_key_id = access_key, aws_secret_access_key = secret_key) self.db_identifier = DBIdentifier def get_manual_snapshots(self): if self.db_identifier == None: return None manual_snapshots = [] for snapshot in self.client.describe_db_snapshots(SnapshotType='manual',DBInstanceIdentifier=self.db_identifier).get('DBSnapshots', None): manual_snapshots.append( (snapshot['SnapshotCreateTime'], snapshot['DBSnapshotIdentifier']) ) manual_snapshots.sort() return manual_snapshots def get_instance_names(self): instance_name_list = [] instances = self.client.describe_db_instances().get('DBInstances', None) #MaxRecords defaults to 100 for instance in instances: instance_name_list.","tags":[],"title":"Boto3 client for AWS RDS","uri":"https://rlmitchell.github.io/2022/11/2022.11.10-boto3-aws-rds-client/","year":"2022"},{"content":" Sometimes you need more that a simple list. In Ansible you can use a list of dictionaries to give each item in the list more than one attribute.\n Here is an example playbook that:\n sets the my_list variable to a list of dictionaries use each of the dictionary attributes in a debug statement  - hosts: localhost become: no gather_facts: no vars: my_list: - id: 'id 1' desc: 'desc 1' - id: 'id 2' desc: 'desc 2' tasks: - name: print out dictionary values debug: msg: \u0026quot;{{ item.id }}, {{ item.desc }}\u0026quot; with_items: \u0026quot;{{ my_list }}\u0026quot;   Here\u0026rsquo;s the output of the playbook run:\n$ ansible-playbook using-list-of-dicts.yml PLAY [localhost] **************************************************************************************************** TASK [debug] ******************************************************************************************************** ok: [localhost] =\u0026gt; (item={'id': 'id 1', 'desc': 'desc 1'}) =\u0026gt; { \u0026quot;msg\u0026quot;: \u0026quot;id 1, desc 1\u0026quot; } ok: [localhost] =\u0026gt; (item={'id': 'id 2', 'desc': 'desc 2'}) =\u0026gt; { \u0026quot;msg\u0026quot;: \u0026quot;id 2, desc 2\u0026quot; } PLAY RECAP ********************************************************************************************************** localhost : ok=1 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 $   The example playbook is also on github.\n","id":6,"section":"posts","summary":"Sometimes you need more that a simple list. In Ansible you can use a list of dictionaries to give each item in the list more than one attribute.\n Here is an example playbook that:\n sets the my_list variable to a list of dictionaries use each of the dictionary attributes in a debug statement  - hosts: localhost become: no gather_facts: no vars: my_list: - id: 'id 1' desc: 'desc 1' - id: 'id 2' desc: 'desc 2' tasks: - name: print out dictionary values debug: msg: \u0026quot;{{ item.","tags":[],"title":"Using a list of dictionaries in Ansible","uri":"https://rlmitchell.github.io/2022/11/2022.11.01-using-list-of-dicts-ansible/","year":"2022"},{"content":" Requirements:  All backup objects in S3 start with a common prefix. All backup objects in S3 have a sortable date/timestamp in the object\u0026rsquo;s name. Keep the most current N backups, delete the rest. Use AWSCLI and Bash so it can be added to cron.   Prerequisites The first thing we need to know how to do is create a sorted array. In this case we need to create the array with the output of another command. In the final script we will created the sorted array from the output of the AWSCLI command listing S3 objects.\n$ array=(`echo \u0026quot;b \u0026gt; c \u0026gt; d \u0026gt; a\u0026quot; | sort | tr '\\n' ' '`) $ declare -p array declare -a array=([0]=\u0026quot;a\u0026quot; [1]=\u0026quot;b\u0026quot; [2]=\u0026quot;c\u0026quot; [3]=\u0026quot;d\u0026quot;) $   Next we need to know how to get the number of elements in an array (so we know when to stop deleting objects).\n$ echo ${#array[@]} 4 $   Finally, we need to know how to remvoe the first element of the sorted array.\n$ echo ${array[*]} a b c d $ array=(\u0026quot;${array[@]:1}\u0026quot;) $ echo ${array[*]} b c d $   Building the script Based on our object name requirements, lets list the backup objects we have in our S3 backups bucket sorting them oldest to newest and assign that output to an array.\nOur backup object name prefix is \u0026ldquo;bck.tf-\u0026rdquo; and there are 12 backups in the bucket.\n# clean-tf-backups.sh version dev.1 S3_BUCKET='s3://mybackupsbucket' OBJECT_PREFIX='bck.tf-' array=(`aws s3 ls ${S3_BUCKET} | awk '{print $NF}' | grep \u0026quot;${OBJECT_PREFIX}\u0026quot; | sort`) declare -p array  output:\n$ bash clean-tf-backups.sh declare -a array=([0]=\u0026quot;bck.tf-20221030.185045.tgz\u0026quot; [1]=\u0026quot;bck.tf-20221030.185048.tgz\u0026quot; [2]=\u0026quot;bck.tf-20221030.185051.tgz\u0026quot; [3]=\u0026quot;bck.tf-20221030.185054.tgz\u0026quot; [4]=\u0026quot;bck.tf-20221030.185057.tgz\u0026quot; [5]=\u0026quot;bck.tf-20221030.185100.tgz\u0026quot; [6]=\u0026quot;bck.tf-20221030.185103.tgz\u0026quot; [7]=\u0026quot;bck.tf-20221030.185106.tgz\u0026quot; [8]=\u0026quot;bck.tf-20221030.185109.tgz\u0026quot; [9]=\u0026quot;bck.tf-20221030.185112.tgz\u0026quot; [10]=\u0026quot;bck.tf-20221030.185115.tgz\u0026quot; [11]=\u0026quot;bck.tf-20221030.185118.tgz\u0026quot;) $   We want to keep the last 10 objects, deleting the oldest first. We will echo \u0026ldquo;delete\u0026rdquo; on each of the objects we want to delete to test/verify.\n# clean-tf-backups.sh version: dev.2 S3_BUCKET='s3://mybackupsbucket' OBJECT_PREFIX='bck.tf-' array=(`aws s3 ls ${S3_BUCKET} | awk '{print $NF}' | grep \u0026quot;${OBJECT_PREFIX}\u0026quot;`) declare array while [[ ${#array[@]} -gt 10 ]] do echo \u0026quot;delete ${array[0]}\u0026quot; array=(\u0026quot;${array[@]:1}\u0026quot;) done for obj in ${array[*]} do echo \u0026quot;keep $obj\u0026quot; done  output:\n$ bash clean-tf-backups.sh delete bck.tf-20221030.185045.tgz delete bck.tf-20221030.185048.tgz keep bck.tf-20221030.185051.tgz keep bck.tf-20221030.185054.tgz keep bck.tf-20221030.185057.tgz keep bck.tf-20221030.185100.tgz keep bck.tf-20221030.185103.tgz keep bck.tf-20221030.185106.tgz keep bck.tf-20221030.185109.tgz keep bck.tf-20221030.185112.tgz keep bck.tf-20221030.185115.tgz keep bck.tf-20221030.185118.tgz $   Finally we add the delete command instead of just echoing.\n# clean-tf-backups.sh version: dev.3 S3_BUCKET='s3://mybackupsbucket' OBJECT_PREFIX='bck.tf-' array=(`aws s3 ls ${S3_BUCKET} | awk '{print $NF}' | grep \u0026quot;${OBJECT_PREFIX}\u0026quot;`) declare array while [[ ${#array[@]} -gt 10 ]] do aws s3 rm ${S3_BUCKET}/${array[0]} echo \u0026quot;deleted ${array[0]}\u0026quot; array=(\u0026quot;${array[@]:1}\u0026quot;) done  output:\n$ bash clean-tf-backups.sh delete: s3://mybackupsbucket/bck.tf-20221030.185045.tgz deleted bck.tf-20221030.185045.tgz delete: s3://mybackupsbucket/bck.tf-20221030.185048.tgz deleted bck.tf-20221030.185048.tgz $   Final script and first run # clean-tf-backups.sh S3_BUCKET='s3://mybackupsbucket' OBJECT_PREFIX='bck.tf-' array=(`aws s3 ls ${S3_BUCKET} | awk '{print $NF}' | grep \u0026quot;${OBJECT_PREFIX}\u0026quot;`) declare array while [[ ${#array[@]} -gt 10 ]] do aws s3 rm ${S3_BUCKET}/${array[0]} array=(\u0026quot;${array[@]:1}\u0026quot;) done   Here\u0026rsquo;s a full run: 1) listing all objects, deleteing old ones with the script, and listing the remaining objects.\n$ aws s3 ls s3://mybackupsbucket | awk '{print $NF}' | grep 'bck.tf-' bck.tf-20221030.185045.tgz bck.tf-20221030.185048.tgz bck.tf-20221030.185051.tgz bck.tf-20221030.185054.tgz bck.tf-20221030.185057.tgz bck.tf-20221030.185100.tgz bck.tf-20221030.185103.tgz bck.tf-20221030.185106.tgz bck.tf-20221030.185109.tgz bck.tf-20221030.185112.tgz bck.tf-20221030.185115.tgz bck.tf-20221030.185118.tgz $ bash clean-tf-backups.sh delete: s3://mybackupsbucket/bck.tf-20221030.185045.tgz delete: s3://mybackupsbucket/bck.tf-20221030.185048.tgz $ $ aws s3 ls s3://mybackupsbucket | awk '{print $NF}' | grep 'bck.tf-' bck.tf-20221030.185051.tgz bck.tf-20221030.185054.tgz bck.tf-20221030.185057.tgz bck.tf-20221030.185100.tgz bck.tf-20221030.185103.tgz bck.tf-20221030.185106.tgz bck.tf-20221030.185109.tgz bck.tf-20221030.185112.tgz bck.tf-20221030.185115.tgz bck.tf-20221030.185118.tgz $  ","id":7,"section":"posts","summary":"Requirements:  All backup objects in S3 start with a common prefix. All backup objects in S3 have a sortable date/timestamp in the object\u0026rsquo;s name. Keep the most current N backups, delete the rest. Use AWSCLI and Bash so it can be added to cron.   Prerequisites The first thing we need to know how to do is create a sorted array. In this case we need to create the array with the output of another command.","tags":[],"title":"Removing old backups in S3 with Bash","uri":"https://rlmitchell.github.io/2022/10/2022.10.30-removing-old-backups-in-s3-with-bash/","year":"2022"},{"content":" The first thing we need is our AWSCLI credentials. Having multiple profiles in the AWSCLI credentials file, we need to set our credentials when we instantiate the boto3 client. So we have a simple dict in a file we will import.\n# boto3_credentials.py boto3_credentials = { 'aws_access_key_id':'\u0026lt;YOUR_ACCESS_KEY_ID\u0026gt;', 'aws_secret_access_key':'\u0026lt;YOUR_SECRET_ACCESS_KEY\u0026gt;', 'region_name':'us-east-2' }   Here is our SESEmail class. It takes a credentials dict, a paramaters dict, and a debug boolean to instantiate. To send the email we simple call the object.\n# ses_email.py import boto3 import subprocess from pprint import pprint class SESEmail: def __init__(self, creds, params, debug=False): self.creds = creds self.__dict__.update(params) self.debug = debug self.client = boto3.client( 'ses', region_name = self.creds['region_name'], aws_access_key_id = self.creds['aws_access_key_id'], aws_secret_access_key = self.creds['aws_secret_access_key'] ) def __call__(self): response = self.send_email() if self.debug: pprint(self.__dict__) pprint(response) def send_email(self): response = self.client.send_email( Destination={'ToAddresses': self.to_address_list }, Message={ 'Body': { 'Text': { 'Charset': 'UTF-8', 'Data': self.body }, }, 'Subject': { 'Charset': 'UTF-8', 'Data': self.subject }, }, Source = self.from_address ) return response   Here is a working example with some information redacted (all caps inside \u0026lt;\u0026gt;)\n$ python3 Python 3.8.10 (default, Jun 22 2022, 20:18:18) [GCC 9.4.0] on linux Type \u0026quot;help\u0026quot;, \u0026quot;copyright\u0026quot;, \u0026quot;credits\u0026quot; or \u0026quot;license\u0026quot; for more information. \u0026gt;\u0026gt;\u0026gt; from boto3_credentials import boto3_credentials as creds \u0026gt;\u0026gt;\u0026gt; from ses_email import SESEmail \u0026gt;\u0026gt;\u0026gt; params = { ... 'from_address':'\u0026lt;FROM@TEST.COM\u0026gt;', ... 'to_address_list':['\u0026lt;TO@TEST.COM\u0026gt;'], ... 'subject':'\u0026lt;SOME SUBJECT STRING\u0026gt;', ... 'body':'\u0026lt;SOME BODY STRING\u0026gt;' ... } \u0026gt;\u0026gt;\u0026gt; SESEmail(creds, params, debug=True)() {'body': '\u0026lt;SOME BODY STRING\u0026gt;', 'client': \u0026lt;botocore.client.SES object at 0x7f0a983dec10\u0026gt;, 'creds': {'aws_access_key_id': '\u0026lt;YOUR_ACCESS_KEY_ID\u0026gt;', 'aws_secret_access_key': '\u0026lt;YOUR_SECRET_ACCESS_KEY\u0026gt;', 'region_name': 'us-east-2'}, 'debug': True, 'from_address': '\u0026lt;FROM@TEST.COM\u0026gt;', 'subject': '\u0026lt;SOME SUBJECT STRING\u0026gt;', 'to_address_list': ['\u0026lt;TO@TEST.COM\u0026gt;']} {'MessageId': '010f01842a8bf60a-4ec8b0ad-963e-4add-83d2-2de748d3011a-000000', 'ResponseMetadata': {'HTTPHeaders': {'connection': 'keep-alive', 'content-length': '326', 'content-type': 'text/xml', 'date': 'Sun, 30 Oct 2022 20:18:46 GMT', 'x-amzn-requestid': '088403a4-7754-4c37-87cb-38041f3e5c95'}, 'HTTPStatusCode': 200, 'RequestId': '088403a4-7754-4c37-87cb-38041f3e5c95', 'RetryAttempts': 0}} \u0026gt;\u0026gt;\u0026gt; quit() $  ","id":8,"section":"posts","summary":"The first thing we need is our AWSCLI credentials. Having multiple profiles in the AWSCLI credentials file, we need to set our credentials when we instantiate the boto3 client. So we have a simple dict in a file we will import.\n# boto3_credentials.py boto3_credentials = { 'aws_access_key_id':'\u0026lt;YOUR_ACCESS_KEY_ID\u0026gt;', 'aws_secret_access_key':'\u0026lt;YOUR_SECRET_ACCESS_KEY\u0026gt;', 'region_name':'us-east-2' }   Here is our SESEmail class. It takes a credentials dict, a paramaters dict, and a debug boolean to instantiate.","tags":[],"title":"Sending email with AWS Simple Email Service (SES) and Python","uri":"https://rlmitchell.github.io/2022/10/2022.10.30-python-send-email-with-aws-ses/","year":"2022"},{"content":" This is super simple but, I mention it because I\u0026rsquo;ve been wanting a way to put comments into some of my markdown documents. I came across this solution today and it seems to work well at least in Hugo.\nUsing link labels to achieve it: [comment]: \u0026lt;\u0026gt; (this is how to make a md comment)\nreference: stackoverflow post\n","id":9,"section":"posts","summary":"This is super simple but, I mention it because I\u0026rsquo;ve been wanting a way to put comments into some of my markdown documents. I came across this solution today and it seems to work well at least in Hugo.\nUsing link labels to achieve it: [comment]: \u0026lt;\u0026gt; (this is how to make a md comment)\nreference: stackoverflow post","tags":[],"title":"How to write comments in markdown","uri":"https://rlmitchell.github.io/2022/10/2022.10.29-markdown-comments/","year":"2022"}],"tags":[]}